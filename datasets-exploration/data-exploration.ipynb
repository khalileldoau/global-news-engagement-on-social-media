{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22188bd-8587-417d-940d-50dbdd7201dc",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "https://www.kaggle.com/datasets/kanchana1990/global-news-engagement-on-social-media?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e46ff-582b-4d7e-90eb-9c64167a2ff9",
   "metadata": {},
   "source": [
    "### Follow the commands in the url below to import the Kaggle dataset into jupyter:\n",
    "https://saturncloud.io/blog/how-to-import-kaggle-datasets-into-jupyter-notebook/#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d12cf-0bc3-43e0-9f13-3a448152f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d kanchana1990/global-news-engagement-on-social-media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546ac99-501d-48b1-94ec-f93599170b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip global-news-engagement-on-social-media.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551c6ae-df01-4d58-8418-9368032e07c7",
   "metadata": {},
   "source": [
    "### Read the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9519e-298c-4a6e-b2fe-2ba4c8b05329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fecec-5b26-4416-9ca0-f0afa568b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aljazeera = pd.read_csv('al_jazeera.csv')\n",
    "df_aljazeera['channel'] = 'aljazeera'\n",
    "\n",
    "df_bbc = pd.read_csv('bbc.csv')\n",
    "df_bbc['channel'] = 'bbc'\n",
    "\n",
    "df_cnn = pd.read_csv('cnn.csv')\n",
    "df_cnn['channel'] = 'cnn'\n",
    "\n",
    "df_reuters = pd.read_csv('reuters.csv')\n",
    "df_reuters['channel'] = 'reuters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96e0cd-0345-4354-af2f-adad9e51f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_aljazeera, df_bbc, df_cnn, df_reuters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f64466-c562-489a-b776-66f5106312f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a62671-7430-4415-87a1-d997989721b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ed6c8-25ce-4239-9454-3654505552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc00d56-5e9c-4ab1-bce5-2b14fb09fa47",
   "metadata": {},
   "source": [
    "### Creating words frequency dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d4e87-cb87-498c-90e7-d9cf7330681b",
   "metadata": {},
   "source": [
    "### Extract topics from data\n",
    "https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8c868-b621-41b6-95cc-63cfd3b53d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cleaning and Preprocessing of the data\n",
    "'''\n",
    "\n",
    "# importing required libraries \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# compile documents\n",
    "doc_complete = df['text'].tolist()\n",
    "\n",
    "#print('\\n\\nData\\n\\n')\n",
    "#print(doc_complete)\n",
    "\n",
    "# set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(str(doc)).split() for doc in doc_complete]    \n",
    "\n",
    "\n",
    "print('\\n\\nCleaned Data\\n\\n')\n",
    "print(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7ce5b-2ce9-436f-b2e0-339a6972d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d41b7-472f-46df-a08a-5ee0717e3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_doc_clean = flatten_extend(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984236ca-f6e3-4b53-81d7-60f6a8722617",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {key: flat_doc_clean.count(key) for key in flat_doc_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791e115-b0ae-4856-af4f-b941d0757274",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e9d80-080e-4408-be49-aab3fe77f04e",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3aee5-eeef-4828-a503-665d6a7a6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad252410-0ed9-44cc-b024-d60719dbb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3fc39-2f18-49c2-adcd-f9f66e4def2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c523138-3ee5-4629-8811-679a011b3185",
   "metadata": {},
   "source": [
    "### Running LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707b068-07ed-4075-b198-426da7afef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a85b7-a8a4-40cb-a109-e6037a5a8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637c2b8-5575-4a8c-8890-bdac1458c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=100, num_words=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
